{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle Component Analysis\n",
    "- **Unsupervised** learning model, meaning that unlike supervised learning models, we are working with unlabeled data.\n",
    "    - You can confirm accuracies with supervised models but cannot with unsupervised models\n",
    "- The PCA plot is used to visualise correlations in a 2d graph \n",
    "- Tight clusters indicate highe correlation\n",
    "- The number of dimenions are reduced so that multiple so that patterns in correlation between features can be easily identified \n",
    "- In other words, PCA aims to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions than the original one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "1. [X] Import your PCA data into a dataframe. Name the columns `PC1`, `PC2`, `PC3`, `PC4`, etc.. \n",
    "2. [X] Drop add columns following `PC3`.\n",
    "3. [ ] Split your scaled data (currently stored in `scaled_X` and `labels`) into training and testing data using `train_test_split()`.  \n",
    "4. [ ] Split your PCA data (currently stored in `X_with_pca` and `labels`) into training and testing sets using `train_test_split()`.\n",
    "5. [ ] Create two `DecisionTreeClassifier` objects. Store one in `pca_clf` and one in `reg_clf`.\n",
    "6. [ ] Fit each model on their respective datasets, and make predictions from each. Compare the accuracy of each. Was the performance of the model fitted using the 2-dimensional PCA data of comparable performance? How would you tell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: *Initialize dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Region</th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12669</td>\n",
       "      <td>9656</td>\n",
       "      <td>7561</td>\n",
       "      <td>214</td>\n",
       "      <td>2674</td>\n",
       "      <td>1338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7057</td>\n",
       "      <td>9810</td>\n",
       "      <td>9568</td>\n",
       "      <td>1762</td>\n",
       "      <td>3293</td>\n",
       "      <td>1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6353</td>\n",
       "      <td>8808</td>\n",
       "      <td>7684</td>\n",
       "      <td>2405</td>\n",
       "      <td>3516</td>\n",
       "      <td>7844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Channel  Region  Fresh  Milk  Grocery  Frozen  Detergents_Paper  Delicassen\n",
       "0        2       3  12669  9656     7561     214              2674        1338\n",
       "1        2       3   7057  9810     9568    1762              3293        1776\n",
       "2        2       3   6353  8808     7684    2405              3516        7844"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "pca_df = pd.read_csv('../datasets/Wholesale customers data.csv')\n",
    "pca_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: *Data Scaling & Transform*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Scaling\n",
    "scaler = StandardScaler(copy = True, with_mean=True, with_std=True)\n",
    "scaler.fit(pca_df)\n",
    "scaled_df = scaler.transform(pca_df)\n",
    "# scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: *PCA Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance for Principal Component 0: 0.3875012291159263\n",
      "Explained Variance for Principal Component 1: 0.2237458795103888\n",
      "Explained Variance for Principal Component 2: 0.1264717345230583\n",
      "Explained Variance for Principal Component 3: 0.09229903718659847\n",
      "Explained Variance for Principal Component 4: 0.06957904970880058\n",
      "Explained Variance for Principal Component 5: 0.0574135443531443\n",
      "Explained Variance for Principal Component 6: 0.03514075682001749\n",
      "Explained Variance for Principal Component 7: 0.007848768782065951\n"
     ]
    }
   ],
   "source": [
    "# Initialise the model\n",
    "pca = PCA() \n",
    "\n",
    "# Fit the model with scaled data\n",
    "pca.fit(scaled_df)\n",
    "\n",
    "# Transform the scaled data\n",
    "pca_transformed = pca.transform(scaled_df)\n",
    "\n",
    "# enumerate through \"pca_transformed.explained_variance_ratio_\"\n",
    "# to see the amount of variance captured by each Principal Component\n",
    "for ind, var in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f'Explained Variance for Principal Component {ind}: {var}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.1: *Dropping Principle Components*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting Principle Components into a Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we see the explained variance we can drop components with low variance\n",
    "##### Columns 0,1 and 2 have the highest variances so we'll keep those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.44865163,  0.59066829,  0.05293319],\n",
       "       [ 1.44865163,  0.59066829, -0.39130197],\n",
       "       [ 1.44865163,  0.59066829, -0.44702926],\n",
       "       ...,\n",
       "       [ 1.44865163,  0.59066829,  0.20032554],\n",
       "       [-0.69029709,  0.59066829, -0.13538389],\n",
       "       [-0.69029709,  0.59066829, -0.72930698]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only take the first three columns as principal components\n",
    "pc_scaled_df = scaled_df[:, 0:4]\n",
    "pc_scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance for Principal Component 0: 0.3897748386022531\n",
      "Explained Variance for Principal Component 1: 0.3445814912712074\n",
      "Explained Variance for Principal Component 2: 0.2656436701265396\n",
      "\n",
      "[0.38977484 0.34458149 0.26564367]\n"
     ]
    }
   ],
   "source": [
    "# Initialise the model\n",
    "pca = PCA() \n",
    "\n",
    "# Fit the model with scaled data\n",
    "pca.fit(pc_scaled_df)\n",
    "\n",
    "# Transform the scaled data\n",
    "pca_transformed = pca.transform(pc_scaled_df)\n",
    "\n",
    "# enumerate through \"pca_transformed.explained_variance_ratio_\"\n",
    "# to see the amount of variance captured by each Principal Component\n",
    "for ind, var in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f'Explained Variance for Principal Component {ind}: {var}') \n",
    "print()\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.01359198,  0.91589454,  0.76423229],\n",
       "       [-1.32508214,  0.7868632 ,  0.47497519],\n",
       "       [-1.36415717,  0.7706768 ,  0.4386892 ],\n",
       "       ...,\n",
       "       [-0.91024296,  0.95870572,  0.86020462],\n",
       "       [ 0.37785955,  0.33841711, -0.7657832 ],\n",
       "       [-0.03858925,  0.16590782, -1.15250737]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "1. [X] Import your PCA data into a dataframe. Name the columns `PC1`, `PC2`, `PC3`, `PC4`, etc.. \n",
    "2. [X] Drop add columns following `PC3`.\n",
    "3. [ ] Split your scaled data (currently stored in `scaled_X` and `labels`) into training and testing data using `train_test_split()`.  \n",
    "4. [ ] Split your PCA data (currently stored in `X_with_pca` and `labels`) into training and testing sets using `train_test_split()`.\n",
    "5. [ ] Create two `DecisionTreeClassifier` objects. Store one in `pca_clf` and one in `reg_clf`.\n",
    "6. [ ] Fit each model on their respective datasets, and make predictions from each. Compare the accuracy of each. Was the performance of the model fitted using the 2-dimensional PCA data of comparable performance? How would you tell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
