{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Unsupervised Learning: Principal Component Analysis & Clustering</h1></center>\n",
    "\n",
    "For this notebook, we'll explore add two important techniques to our Data Science toolbox: **_Principal Component Analysis (PCA)_** and **_Clustering_**.  Unlike all the Supervised Learning techniques we've used to far, these two are unique because they are examples of **_Unsupervised Learning_**.  Whereas we require labeled data to double check the accuracy of algorithms like Decision Trees and Naive Bayesian Classifiers, these techniques work on unlabeled data.  While this makes it much easier to apply these techniques to many more kinds of data, it also means that we have no way to measure how well the algorithm is or isn't working.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Challenge 1: Apply PCA to the Iris Data Set</h2></center> \n",
    "\n",
    "To help us explore the concept of PCA, we're going to start by applying PCA to the _Iris Dataset_.  We'll then use it to fit a model and classify the flower types as we have done in previous examples.\n",
    "\n",
    "\n",
    " Before we begin, watch this [primer from StatQuest](https://www.youtube.com/embed/_UVHneBUBW0) to understand what PCA, and read this [short (interactive) article](http://setosa.io/ev/principal-component-analysis/) about using PCA on the Iris dataset.  These two examples should help you better understand how PCA works, and more importantly, how it can be useful to you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first challenge, we'll import the _Iris Dataset_ from `sklearn.datasets` and use PCA on it. By examining the explained variance of Principal Components, we'll see that we can actually drop 1 or 2 columns (reducing our **_dimensionality_**) while only losing a minimal amount of predictive accuracy.  \n",
    "\n",
    "Follow these steps in the code block below:\n",
    "\n",
    "1. Call `load_iris()` and store the results in the `iris` variable. \n",
    "<br>\n",
    "<br>\n",
    "1. Create a `StandardScaler()` object and store it in `scaler`.\n",
    "<br>\n",
    "<br>\n",
    "1. Call `scaler.fit()` on `iris.data`, and then use `scaler.transform` to create a scaled version of your data.  Store the results in `scaled_x`.\n",
    "<br>\n",
    "<br>\n",
    "1. Store the labels for _iris_ `labels`.\n",
    "<br>\n",
    "<br>\n",
    "1.  Create a `PCA()` object and store it in `pca`.  Fit it to the scaled data using `pca.fit()`.  Then, call `pca.transform()` on `scaled_x` and store the results in `X_with_pca`.\n",
    "<br>\n",
    "<br>\n",
    "1. Complete the `enumerate` statement to to enumerate through `pca.explained_variance_ratio_` and print out the variance captured by each of the Principal Components.\n",
    "\n",
    "If you followed these steps correctly, you have will have now created 4 _Principal Components_ from your original dataset.  Be sure to use the information printed out by running the cell below to answer the following questions below it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance for Principal Component 0: 0.7296244541329986\n",
      "Explained Variance for Principal Component 1: 0.22850761786701776\n",
      "Explained Variance for Principal Component 2: 0.03668921889282878\n",
      "Explained Variance for Principal Component 3: 0.005178709107154798\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "scaler = StandardScaler()\n",
    "#Fit the scaler to iris.data\n",
    "\n",
    "# call scaler.transform() on iris.data and store the result in scaled_X\n",
    "scaled_X = scaler.fit_transform(iris.data)\n",
    "\n",
    "# Store the labels contained in iris.targets below\n",
    "labels = iris.target\n",
    "\n",
    "# Create a PCA() object\n",
    "pca = PCA()\n",
    "\n",
    "#Fit the pca object to scaled_X\n",
    "pca.fit(scaled_X)\n",
    "\n",
    "# Call pca.transform() on scaled_X and store the results below\n",
    "X_with_pca = pca.transform(scaled_X)\n",
    "\n",
    "# Enumerate through pca.explained_variance_ratio_ to see the amount of variance captured by each Principal Component\n",
    "for ind, var in enumerate(pca.explained_variance_ratio_):\n",
    "    print(\"Explained Variance for Principal Component {}: {}\".format(ind, var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Understanding our Results</h3></center>\n",
    "<br>\n",
    "<br>\n",
    "<center>Challenge: Use your results from above to answer the following questions:</center>\n",
    "<br>\n",
    "<br>\n",
    "<center>1) Complete the following table using your results from above.</center>\n",
    "\n",
    "| Principal Component | Variance Explained  |\n",
    "|---------------------|---------------------|\n",
    "|      PC1               |0.7296244541329986|\n",
    "|      PC2            |0.22850761786701776|\n",
    "|      PC3               |0.03668921889282878|\n",
    "|      PC4            |0.005178709107154798|\n",
    "\n",
    "<center>2) Based on the explained variances in the table above, do you recommend dropping any of the columns to reduce dimensionality? Explain your answer.</center>\n",
    "\n",
    "<center><b>Answer:</b> We could drom the third and fourth columns because they have low variances. This means that without them the results won't change drastically.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Challenge: Fit a model using using Principal Components</h3></center>\n",
    "\n",
    "Using the data from above, complete the following steps:\n",
    "\n",
    "1.  Import your PCA data into a dataframe. Name the columns `PC1`, `PC2`, `PC3`, and `PC4`.\n",
    "1.  Drop `PC3` and `PC4` columns.\n",
    "1.  Split your scaled data (currently stored in `scaled_X` and `labels`) into training and testing data using `train_test_split()`.\n",
    "1.  Split your PCA data (currently stored in `X_with_pca` and `labels`) into training and testing sets using `train_test_split()`\n",
    "1.  Create two `DecisionTreeClassifier` objects.  Store one in `pca_clf` and one in `reg_clf`.\n",
    "1.  Fit each model on their respective datasets, and make predictions from each.  Compare the accuracy of each. Was the performance of the model fitted using the 2-dimensional PCA data of comparable performance? How would you tell.  \n",
    "\n",
    "**_Stretch Challenge:_** Use `K-Fold Cross Validation` on each to run the models multiple times and get an average performance for each.  Try this with K >= 5.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for regular model: 0.9210526315789473\n",
      "Accuracy for model with PCA: 0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "# fancy syntax to keep only the first 2 columns\n",
    "X_with_pca = X_with_pca[:, 0:2]\n",
    "\n",
    "pca_X_train, pca_X_test, pca_y_train, pca_y_test = train_test_split(X_with_pca, labels)\n",
    "reg_X_train, reg_X_test, reg_y_train, reg_y_test = train_test_split(scaled_X, labels)\n",
    "\n",
    "# Fit both models on the appropriate datasets\n",
    "reg_clf = DecisionTreeClassifier()\n",
    "reg_clf = reg_clf.fit(reg_X_train, reg_y_train)\n",
    "\n",
    "pca_clf = DecisionTreeClassifier()\n",
    "pca_clf = pca_clf.fit(pca_X_train, pca_y_train)\n",
    "\n",
    "\n",
    "# Use each fitted model to make predictions on the appropriate test sets\n",
    "reg_pred = reg_clf.predict(reg_X_test)\n",
    "pca_pred = pca_clf.predict(pca_X_test)\n",
    "\n",
    "print(\"Accuracy for regular model: {}\".format(accuracy_score(reg_y_test, reg_pred)))\n",
    "print(\"Accuracy for model with PCA: {}\".format(accuracy_score(pca_y_test, pca_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>What is PCA?</h3></center>\n",
    "\n",
    "**_TASK:_** Answer the following questions about PCA based on what you learned from class, the video, and the reading listed above. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<b>Q1: </b>How would you explain how PCA works to someone non-technical?  \n",
    "<b>Answer: </b>PCA is a dimensionality reduction algorithm (it helps you get rid of less necessary features). It can tell you the variance (or how far apart the data is spread) of each dimensions in your data. From this it can give you a score on how much of your data is preserved in each feature and ones with less variance can be dropped. This will make your model far more efficient for little cost.\n",
    "<br>\n",
    "<br>\n",
    "<b>Q2: </b>In what way(s) can PCA be useful in Data Science and Machine Learning? Provide at least 2 examples.  \n",
    "<b>Answer: </b>For making models more efficient. For dropping unnecessary columns.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Challenge 2: Apply PCA and Clustering to Wholesale Customer Data</h2></center>\n",
    "Challenge 2: Apply PCA to [Wholesale Customers Dataset](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers), which we'll get from the UCI Machine Learning Datasets repository.  This dataset contains the purchase records from clients of a wholesale distributor.  It details the total annual purchases across categories seen in the data dictionary below:\n",
    "\n",
    "**Category** | **Description** \n",
    ":-----:|:-----:\n",
    "CHANNEL| 1= Hotel/Restaurant/Cafe, 2=Retailer (Nominal)\n",
    "REGION| Geographic region of Portugal for each order (Nominal)\n",
    "FRESH| Annual spending (m.u.) on fresh products (Continuous);\n",
    "MILK| Annual spending (m.u.) on milk products (Continuous); \n",
    "GROCERY| Annual spending (m.u.)on grocery products (Continuous); \n",
    "FROZEN| Annual spending (m.u.)on frozen products (Continuous) \n",
    "DETERGENTS\\_PAPER| Annual spending (m.u.) on detergents and paper products (Continuous) \n",
    "DELICATESSEN| Annual spending (m.u.)on and delicatessen products (Continuous); \n",
    "\n",
    "**_TASK:_** Read in `wholesale_customers_data.csv` from the `datasets` folder and store in a dataframe.  Store the `Channel` column in a separate variable, and then drop the `Channel` and `Region` columns from the dataframe. Scale the data and use PCA to engineer new features (Principal Components).  Print out the explained variance for each principal component.  Be sure to make your code portable--we'll be using this in our next Jupyter Notebook on K-Means Clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12669</td>\n",
       "      <td>9656</td>\n",
       "      <td>7561</td>\n",
       "      <td>214</td>\n",
       "      <td>2674</td>\n",
       "      <td>1338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7057</td>\n",
       "      <td>9810</td>\n",
       "      <td>9568</td>\n",
       "      <td>1762</td>\n",
       "      <td>3293</td>\n",
       "      <td>1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6353</td>\n",
       "      <td>8808</td>\n",
       "      <td>7684</td>\n",
       "      <td>2405</td>\n",
       "      <td>3516</td>\n",
       "      <td>7844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13265</td>\n",
       "      <td>1196</td>\n",
       "      <td>4221</td>\n",
       "      <td>6404</td>\n",
       "      <td>507</td>\n",
       "      <td>1788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22615</td>\n",
       "      <td>5410</td>\n",
       "      <td>7198</td>\n",
       "      <td>3915</td>\n",
       "      <td>1777</td>\n",
       "      <td>5185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fresh  Milk  Grocery  Frozen  Detergents_Paper  Delicassen\n",
       "0  12669  9656     7561     214              2674        1338\n",
       "1   7057  9810     9568    1762              3293        1776\n",
       "2   6353  8808     7684    2405              3516        7844\n",
       "3  13265  1196     4221    6404               507        1788\n",
       "4  22615  5410     7198    3915              1777        5185"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/Wholesale customers data.csv')\n",
    "# Now Drop the Channel and Region Columns\n",
    "df = df.drop(['Channel', 'Region'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
